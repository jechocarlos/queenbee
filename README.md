# QueenBee ğŸ

**Meta-Agent Orchestration System with Specialized Thinking Agents**

![Tests](https://img.shields.io/badge/tests-223%20passed%2C%202%20skipped-brightgreen)
![Coverage](https://img.shields.io/badge/coverage-70%25-yellow)
![Python](https://img.shields.io/badge/python-3.14-blue)
![License](https://img.shields.io/badge/license-MIT-blue)

QueenBee is an intelligent agent orchestration system that dynamically spawns specialized thinking agents to tackle complex problems through divergent exploration, convergent synthesis, and critical analysis.

## âœ¨ Key Features

- **ğŸ¯ Pure Orchestration**: Queen agent delegates all requests to specialist team
- **ğŸ§  Multi-Agent Collaboration**: Four specialist agents work together on every query
  - **Divergent Agent**: Explores multiple perspectives and creative approaches
  - **Convergent Agent**: Synthesizes insights and ranks recommendations
  - **Critical Agent**: Validates solutions and identifies risks
  - **Summarizer Agent**: Generates rolling summaries and final synthesis
- **ğŸ“Š Live Progress Updates**: Real-time rolling summaries show emerging insights
- **âš¡ Real-time Streaming**: LLM responses stream token-by-token for immediate feedback
- **ğŸ’¬ Live Chat History**: View entire conversation with `history` command
- **ğŸ”„ Async Worker Processes**: Specialists run in background for true parallelism
- **ğŸ—„ï¸ Persistent Memory**: PostgreSQL-backed agent state, tasks, and knowledge
  - Robust connection management with automatic reconnection
  - Transaction safety with commit/rollback
  - Context manager support for clean resource handling
- **ğŸ¤– Flexible LLM Backend**: Choose your AI provider
  - **Ollama**: Local LLM for privacy and control
  - **OpenRouter**: Access to Claude, GPT-4, and other premium models
- **â° Activity-Based TTL**: Agents expire after inactivity, keeping system lean

## Architecture

```
User Input
    â†“
Queen Agent (Pure Orchestrator)
    â†“
Task Queue â†’ Specialist Discussion
                â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“          â†“          â†“
 Divergent  Convergent  Critical
 (Explore)  (Synthesize) (Validate)
     â†“          â†“          â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        Summarizer Agent
     (Rolling + Final Synthesis)
                â†“
      Queen's Final Response
                â†“
                    Collaborative Response
```

## Quick Start

### Prerequisites

- Python 3.14+
- Docker & Docker Compose
- Ollama (or use containerized version)

### Installation

1. **Clone the repository**:
```bash
git clone https://github.com/jechocarlos/queenbee.git
cd queenbee
```

2. **Set up environment**:
```bash
cp .env.example .env
# Edit .env with your configuration
```

3. **Start infrastructure** (local mode):
```bash
docker-compose -f docker-compose.local.yml up -d
```

4. **Pull Ollama model**:
```bash
docker exec -it queenbee-ollama ollama pull llama3.1:8b
```

5. **Install Python dependencies**:
```bash
pip install -e .
```

6. **Run migrations**:
```bash
python scripts/migrate.py
```

7. **Start QueenBee**:

**With Ollama (local):**
```bash
queenbee
```

**With OpenRouter (cloud):**
```bash
# Add your OpenRouter API key to .env
echo "OPENROUTER_API_KEY=your_key_here" >> .env

# Run with OpenRouter
queenbee-openrouter
```

## Usage

### Every Request Gets Full Team Analysis
```
> What's the capital of France?

[Queen] Delegating to my specialist team for collaborative analysis...

ğŸ”µ Divergent #1
France's capital is Paris, known for the Eiffel Tower, Louvre Museum, and as a cultural hub...

ğŸŸ¢ Convergent #1
Paris is definitively the capital of France, both politically and culturally...

ğŸ”´ Critical #1
Confirmed: Paris has been France's capital since 987 CE...

ğŸ“‹ FINAL SYNTHESIS
Paris is the capital of France, serving as the political, cultural, and economic center.

ğŸ QUEEN'S RESPONSE
Paris is the capital of France.
```

### Complex Analysis Example
```
> Should I use microservices or a monolith for my startup?

[Queen] Delegating to my specialist team for collaborative analysis...

ğŸ”µ Divergent #1
Let's explore multiple architectural approaches: microservices for scalability...

ğŸŸ¢ Convergent #1
Synthesizing the perspectives: start with a modular monolith...

ğŸ”´ Critical #1
Key concerns: operational complexity vs development speed...

ğŸ’­ Rolling Summary
The team is analyzing trade-offs between microservices (scalability, complexity) 
and monoliths (simplicity, faster initial development)...

[Discussion continues...]

ğŸ“‹ FINAL SYNTHESIS
For a startup, begin with a modular monolith to validate product-market fit quickly. 
Extract microservices only when specific scaling bottlenecks emerge. Key trade-off: 
operational complexity vs development velocity.

ğŸ QUEEN'S RESPONSE
I recommend starting with a modular monolith architecture for your startup...
```

[Queen] Analysis complete. Here's what the team found:
[Synthesized findings from all three specialists]
```

## Configuration

### Environment Variables (`.env`)

```bash
# Common
LOG_LEVEL=INFO

# Ollama (local LLM)
OLLAMA_HOST=http://localhost:11434

# OpenRouter (cloud LLM - optional)
OPENROUTER_API_KEY=your_api_key_here

# Database (local deployment)
DB_HOST=postgres
DB_PORT=5432
DB_NAME=queenbee
DB_USER=queenbee
DB_PASSWORD=your_password

# Database (remote deployment)
# DB_HOST=your-remote-host.com
# DB_SSL_MODE=require
```

### System Configuration (`config.yaml`)

See `config.yaml` for full configuration options including:
- Agent TTL settings
- Consensus parameters
- **Ollama**: Local model selection (llama3.1:8b by default)
- **OpenRouter**: Cloud model selection (anthropic/claude-3.5-sonnet by default)
- Specialist behavior tuning

### LLM Provider Selection

**Use Ollama (local, private, free):**
```bash
queenbee
```

**Use OpenRouter (cloud, premium models):**
```bash
queenbee-openrouter
```

Both commands use the same configuration and database - only the LLM provider changes. You can switch between providers anytime without losing conversation history.

### Rate Limiting (OpenRouter)

OpenRouter integration includes intelligent rate limiting:

```yaml
# config.yaml
openrouter:
  requests_per_minute: 16  # Free tier limit
  max_retries: 3           # Retry on rate limit
  retry_delay: 5           # Seconds between retries
```

**How it works:**
- **Token Bucket Algorithm**: Automatically throttles requests to respect provider limits
- **Proactive Limiting**: Prevents hitting rate limits by tracking usage locally
- **Automatic Retries**: Handles 429 errors with exponential backoff
- **Thread-Safe**: Works correctly with multiple specialist agents running concurrently

**Rate Limit Tiers:**
- **Free**: 16 requests/minute (default)
- **Paid Plans**: Update `requests_per_minute` in config to match your tier

## Deployment Modes

### Local Development
All services containerized:
```bash
docker-compose -f docker-compose.local.yml up -d
```

### Remote Database
Ollama only, connect to external PostgreSQL:
```bash
docker-compose -f docker-compose.remote.yml up -d
```

## Project Structure

```
queenbee/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ SPECS.md          # System specification
â”‚   â””â”€â”€ TODO.md           # Development tasks
â”œâ”€â”€ src/queenbee/
â”‚   â”œâ”€â”€ agents/           # Agent implementations
â”‚   â”œâ”€â”€ cli/              # Command-line interface
â”‚   â”œâ”€â”€ config/           # Configuration management
â”‚   â”œâ”€â”€ db/               # Database layer
â”‚   â””â”€â”€ session/          # Session management
â”œâ”€â”€ prompts/              # Agent system prompts
â”œâ”€â”€ migrations/           # Database migrations
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ config.yaml           # System configuration
â”œâ”€â”€ .env.example          # Environment template
â””â”€â”€ pyproject.toml        # Python package config
```

## Development

### Running Tests
```bash
pytest                              # Run all tests
pytest -v                          # Verbose output
pytest --cov                       # With coverage report
pytest tests/test_agents.py        # Specific test file
```

**Latest Test Results:**
- âœ… 223 tests passed
- âš ï¸ 2 tests skipped (environment-dependent)
- ğŸ“Š 70% code coverage
- â±ï¸ Completed in 173 seconds (2m 53s)

### Code Quality
```bash
black src/ tests/
ruff check src/ tests/
mypy src/
```

### Database Migrations
```bash
python scripts/migrate.py
```

## Specialist Thinking Modes

### Queen Agent (Pure Orchestrator)
- Receives all user requests
- Delegates to specialist team for analysis
- Receives synthesis from SummarizerAgent
- Presents final response to user
- No direct analysis or complexity checking

### Divergent Thinker
- Explores possibilities
- Generates multiple approaches
- Challenges assumptions
- Identifies edge cases

### Convergent Thinker
- Evaluates options
- Creates action plans
- Prioritizes solutions
- Resolves contradictions

### Critical Thinker
- Identifies flaws
- Assesses risks
- Validates assumptions
- Tests robustness

### Summarizer Agent
- Generates rolling summaries during discussion
- Synthesizes insights from all perspectives
- Creates final comprehensive synthesis
- Focuses on substance over process

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

MIT License - see LICENSE file for details

## Roadmap

- [x] Phase 1: MVP with single specialist
- [ ] Phase 2: Full consensus protocol
- [ ] Phase 3: Production polish
- [ ] Web UI interface
- [ ] Multi-user support
- [ ] Agent learning persistence

## Support

- Issues: https://github.com/jechocarlos/queenbee/issues
- Documentation: See `/docs` directory
- Specification: See `.github/SPECS.md`

---

Built with â¤ï¸ using Python, PostgreSQL, and Ollama