# Streaming and Chat History Features

## Overview

QueenBee now supports **real-time streaming responses** and **live chat history tracking**.

## Features

### 1. Streaming LLM Inference

**What it does:**
- Responses are displayed token-by-token as they're generated by Ollama
- Provides immediate feedback instead of waiting for complete responses
- Creates a more interactive, responsive experience

**Technical details:**
- `BaseAgent.generate_response()` now accepts `stream=bool` parameter
- Returns `Iterator[str]` when streaming is enabled
- CLI automatically handles streaming display with `stream_response()` function

**Example usage:**
```python
# In agent code
response = self.generate_response(prompt, stream=True)

# Returns an iterator that yields chunks:
# "Hello" -> " there" -> "! How" -> " can" -> " I" -> " help?"
```

### 2. Live Chat History

**What it does:**
- All messages (user and Queen) are automatically saved to the database
- Type `history` command to view recent conversation
- Shows message count after each interaction
- Displays up to 20 most recent messages in a formatted table

**Available commands:**
- `history` - Show recent chat history
- `hist` - Shortcut for history
- `h` - Even shorter shortcut

**What's tracked:**
- User messages (role: USER)
- Queen responses (role: QUEEN)
- Agent ID (which agent generated the response)
- Timestamps (automatically added by database)
- Session ID (groups conversations together)

### 3. Visual Improvements

**Status indicators:**
```
ğŸ’¬ Total messages in session: 6 (type 'history' to view)
```

**History display:**
```
ğŸ“œ Recent Chat History:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Role        â”‚ Message                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ USER        â”‚ What is Python?                            â”‚
â”‚ ğŸ QUEEN    â”‚ Python is a high-level programming lang... â”‚
â”‚ USER        â”‚ How do I install it?                       â”‚
â”‚ ğŸ QUEEN    â”‚ You can install Python from python.org... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture Changes

### Database Schema
No changes needed - uses existing `chat_history` table:
- `id`: Message ID
- `session_id`: Links to current session
- `agent_id`: Which agent sent the message
- `role`: USER, QUEEN, DIVERGENT, CONVERGENT, or CRITICAL
- `content`: Message text
- `timestamp`: When message was created
- `metadata`: JSON field for additional data

### Code Changes

**1. BaseAgent (`src/queenbee/agents/base.py`)**
- Added `stream: bool` parameter to `generate_response()`
- Return type changed to `Union[str, Iterator[str]]`

**2. QueenAgent (`src/queenbee/agents/queen.py`)**
- `process_request()` now accepts `stream: bool` parameter
- Returns `Union[str, Iterator[str]]`
- For complex requests with streaming, uses generator to combine placeholder + streamed response
- Chat history logging moved to CLI (when streaming, can't log until complete)

**3. CLI (`src/queenbee/cli/main.py`)**
- New `display_chat_history()` function - Shows recent messages in table format
- New `stream_response()` function - Handles real-time token display
- Main loop updated to:
  - Pass `stream=True` to `queen.process_request()`
  - Handle both string and iterator responses
  - Log complete streamed responses to database
  - Show message count after each interaction
- Added `history` command support

## Usage Examples

### Starting QueenBee
```bash
queenbee
```

### Basic conversation with streaming
```
You: What is machine learning?
Queen is thinking...

ğŸ Queen: Machine learning is a subset of artificial intelligence...
[response streams in real-time, token by token]

ğŸ’¬ Total messages in session: 2 (type 'history' to view)
```

### Viewing history
```
You: history

ğŸ“œ Recent Chat History:
[Shows table with all messages in current session]
```

### Complex requests
For complex questions, Queen still shows the "[QueenBee] This is a complex..." notice, but now streams the actual response afterward in real-time.

## Performance Considerations

**Streaming:**
- Reduces perceived latency (users see immediate progress)
- No memory overhead (tokens processed as received)
- Network-efficient (uses HTTP chunked transfer encoding)

**Chat history:**
- Database write per message (minimal overhead)
- `get_session_history()` uses indexed queries (fast retrieval)
- Optional `limit` parameter prevents loading thousands of messages

## Future Enhancements

When specialist agents are implemented (Phase 2):
- Chat history will show messages from all agents (Divergent, Convergent, Critical)
- Can filter history by agent type
- Stream multiple agents' responses simultaneously
- Real-time collaborative thinking visualization

## Configuration

No configuration changes needed. Streaming is always enabled by default in the CLI.

To disable streaming (for programmatic use):
```python
response = queen.process_request(user_input, stream=False)
```

## Troubleshooting

**Streaming not working?**
- Check Ollama is running: `ollama list`
- Verify Ollama version supports streaming (most recent versions do)
- Check network connectivity to Ollama host

**History not showing?**
- Verify migrations are applied: `python scripts/migrate.py`
- Check database connection: `python scripts/test_setup.py`
- Ensure messages are being logged (check for errors in logs)

**Slow streaming?**
- Ollama performance depends on model size and hardware
- Smaller models (7B) stream faster than larger ones (70B)
- GPU acceleration significantly improves streaming speed
