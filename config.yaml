system:
  name: queenbee
  version: 1.0.0
  environment: development

database:
  host: ${DB_HOST:localhost}
  port: ${DB_PORT:5432}
  name: ${DB_NAME:queenbee}
  user: ${DB_USER:queenbee}
  password: ${DB_PASSWORD}
  ssl_mode: ${DB_SSL_MODE:prefer}

ollama:
  host: ${OLLAMA_HOST:http://localhost:11434}
  model: gpt-oss:20b
  timeout: 300

openrouter:
  api_key: ${OPENROUTER_API_KEY:}
  model: openai/gpt-oss-20b
  timeout: 300
  base_url: https://openrouter.ai/api/v1
  verify_ssl: false  # Disable SSL verification (for testing with corporate proxies)
  
  # Rate limiting (free tier: 16 requests/min)
  requests_per_minute: 16
  max_retries: 3
  retry_delay: 5  # seconds

# Inference Packs: Define model configurations for different use cases
inference_packs:
  packs:
    reasoning:
      model: openai/gpt-oss-20b
      provider: openrouter
      extract_reasoning: true
      temperature: 0.5
      max_tokens: 500
    
    standard:
      model: openai/gpt-oss-20b
      provider: openrouter
      extract_reasoning: false  # Use standard content field
      temperature: 0.5
      max_tokens: 500
    
    web_search:
      model: openai/gpt-4o-mini-search-preview
      provider: openrouter
      extract_reasoning: false
      temperature: 0.1
      max_tokens: 800
    
    fast:
      model: meta-llama/llama-3.1-8b-instruct:free
      provider: openrouter
      extract_reasoning: false
      temperature: 0.3
      max_tokens: 300
    
    local:
      model: gpt-oss-20b
      provider: ollama
      extract_reasoning: false
      temperature: 0.7
      max_tokens: 500

# Agent Inference Assignments: Map agents to inference packs
agent_inference:
  queen: standard
  divergent: web_search
  convergent: reasoning
  critical: standard
  summarizer: standard

agents:
  ttl:
    idle_timeout_minutes: 10
    check_interval_seconds: 30
  
  max_concurrent_specialists: 10
  
  queen:
    system_prompt_file: ./prompts/queen.md
    complexity_threshold: auto
  
  divergent:
    system_prompt_file: ./prompts/divergent.md
    max_iterations: 25
    max_tokens: 500  # Maximum tokens per contribution
  
  convergent:
    system_prompt_file: ./prompts/convergent.md
    max_iterations: 25
    max_tokens: 500  # Maximum tokens per contribution
  
  critical:
    system_prompt_file: ./prompts/critical.md
    max_iterations: 25
    max_tokens: 500  # Maximum tokens per contribution
  
  summarizer:
    system_prompt_file: ./prompts/summarizer.md
    max_iterations: 25
    max_tokens: 0  # No limit for summarizer

consensus:
  max_rounds: 20
  agreement_threshold: "all"
  discussion_rounds: 20  # Number of rounds for iterative agent discussion
  specialist_timeout_seconds: 300  # Maximum wait time for specialists (5 minutes)
  summary_interval_seconds: 10  # Update rolling summary every 10 seconds

logging:
  level: ${LOG_LEVEL:INFO}
  format: json
  output: stdout
