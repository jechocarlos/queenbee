system:
  name: queenbee
  version: 1.0.0
  environment: development

database:
  host: ${DB_HOST:localhost}
  port: ${DB_PORT:5432}
  name: ${DB_NAME:queenbee}
  user: ${DB_USER:queenbee}
  password: ${DB_PASSWORD}
  ssl_mode: ${DB_SSL_MODE:disable}  # Use "require" for remote databases

ollama:
  host: ${OLLAMA_HOST:http://localhost:11434}
  model: gpt-oss:20b
  timeout: 300

openrouter:
  api_key: ${OPENROUTER_API_KEY:}
  model: openai/gpt-oss-20b
  timeout: 300
  base_url: https://openrouter.ai/api/v1
  verify_ssl: false  # Disable SSL verification (for testing with corporate proxies)
  
  # Rate limiting (free tier: 16 requests/min)
  requests_per_minute: 16
  max_retries: 3
  retry_delay: 5  # seconds

# Inference Packs: Define model configurations grouped by provider
inference_packs:
  # Provider-specific configurations
  openrouter:
    default_pack: standard  # Default pack for OpenRouter agents
    packs:
      reasoning:
        model: openai/gpt-oss-20b
        extract_reasoning: true
        temperature: 0.3
        max_tokens: 3000
      
      standard:
        model: qwen/qwen3-vl-8b-instruct
        extract_reasoning: false
        temperature: 0.0
        max_tokens: 2000
      
      web_search:
        model: openai/gpt-4o-mini-search-preview
        extract_reasoning: false
        temperature: 0.1
        max_tokens: 5000
      
      fast:
        model: google/gemini-2.5-flash-lite
        extract_reasoning: false
        temperature: 0.1
        max_tokens: 500
  
  ollama:
    default_pack: standard  # Default pack for Ollama agents
    packs:
      standard:
        model: llama3.1:8b
        extract_reasoning: false
        temperature: 0.7
        max_tokens: 2000
      
      reasoning:
        model: llama3.1:8b
        extract_reasoning: false
        temperature: 0.5
        max_tokens: 2000
      
      fast:
        model: qwen2.5:3b
        extract_reasoning: false
        temperature: 0.3
        max_tokens: 2000
      
      # Note: web_search pack not available for Ollama
      # If an agent requests 'web_search' with Ollama, it will fall back to 'standard'

# Agent Inference Assignments: Map agents to inference packs (pack names only)
# Provider is determined by command used (queenbee = ollama, queenbee-openrouter = openrouter)
# If pack not found in provider, falls back to provider's default_pack
# If default_pack also unavailable, agent initialization fails
agent_inference:
  queen: standard
  classifier: fast
  divergent: reasoning
  convergent: reasoning
  critical: standard
  pragmatist: reasoning
  user_proxy: standard
  quantifier: reasoning
  summarizer: standard
  web_searcher: web_search  # Only works with OpenRouter


agents:
  ttl:
    idle_timeout_minutes: 10
    check_interval_seconds: 30
  
  max_concurrent_specialists: 10
  
  queen:
    system_prompt_file: ./prompts/queen.md
    complexity_threshold: auto
    simple_max_tokens: 100
    complex_max_tokens: 8000
  
  classifier:
    system_prompt_file: ./prompts/classifier.md
    max_tokens: 10  # Very short response needed (just SIMPLE or COMPLEX)
  
  divergent:
    system_prompt_file: ./prompts/divergent.md
    max_iterations: 25
    max_tokens: 2000  # Maximum tokens per contribution
  
  convergent:
    system_prompt_file: ./prompts/convergent.md
    max_iterations: 25
    max_tokens: 2000  # Maximum tokens per contribution
  
  critical:
    system_prompt_file: ./prompts/critical.md
    max_iterations: 25
    max_tokens: 2000  # Maximum tokens per contribution
  
  pragmatist:
    system_prompt_file: ./prompts/pragmatist.md
    max_iterations: 25
    max_tokens: 2000  # Maximum tokens per contribution
  
  user_proxy:
    system_prompt_file: ./prompts/user_proxy.md
    max_iterations: 25
    max_tokens: 2000  # Maximum tokens per contribution
  
  quantifier:
    system_prompt_file: ./prompts/quantifier.md
    max_iterations: 25
    max_tokens: 2000  # Maximum tokens per contribution
  
  summarizer:
    system_prompt_file: ./prompts/summarizer.md
    max_iterations: 25
    max_tokens: 0  # No limit for summarizer
  
  web_searcher:
    system_prompt_file: ./prompts/web_searcher.md
    max_tokens: 0  # More tokens for search results
consensus:
  max_rounds: 30
  agreement_threshold: "all"
  discussion_rounds: 30  # Number of rounds for iterative agent discussion
  specialist_timeout_seconds: 300  # Maximum wait time for specialists (5 minutes)
  summary_interval_seconds: 15  # Update rolling summary every 15 seconds

logging:
  level: ${LOG_LEVEL:INFO}
  format: json
  output: stdout
